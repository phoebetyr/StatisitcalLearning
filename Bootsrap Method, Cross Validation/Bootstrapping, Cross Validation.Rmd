---
title: "Validation Set Approach"
author: "Phoebe Tan"
date: "`r Sys.Date()`"
output: html_document
---
#The Validation Set Approach:

The validation set approach is a method for evaluating the performance of a predictive model in statistical modeling and machine learning. The approach involves splitting the dataset into two parts: a training set and a validation set. The training set is used to train the model, while the validation set is used to evaluate the performance of the model. 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

## Preprocessing Data
```{r}
#Data from https://www.kaggle.com/datasets/jmcaro/wheat-seedsuci
library(tidyverse)
library(class)
library(dplyr)
data= read.csv("seeds.csv")
str(data)
head(data)
unique(data$Type) #Target Output: Type of Wheat seeds- 1,2,3
```


#mutate()
-What it does: Adds new columns or modifies current variables in the dataset.
```{r}
data=mutate(data,class=0)
data
str(data)
```

```{r}
nrow(data) # total number of row in the dataset train
nrow(data[data$Type==1,]) # 66 rows of seed type 1
nrow(data[data$Type==2,]) # 68 rows of seed type 1
nrow(data[data$Type==3,]) # 65 rows of seed type 1


data$class[data$Type==1]=1 #assign class 1 to seed type 1
data$class[data$Type==2]=2 #assign class 2 to seed type 2
data$class[data$Type==3]=3 #assign class 3 to seed type 3
data
```


```{r}

data=data[,-c(2,4,6,8)]   # removed these columns from the dataset
data
```

#The Validation Set Approach:-

# spliting the dataset into training and validation
```{r}
set.seed(10)
index=sample(1:nrow(data),0.75*nrow(data),replace = F) #replace = FALSE: This indicates that the sample is generated without replacement. In other words, each number can only appear once in the sample.
index
trainingset=data[index,]  # randomly selected 104 dataset for traning 
validaionset=data[-index,]  # randomly selected 44 dataset for validation
validaionset
```

# Traning and validating the model 
```{r}
model=knn(train = trainingset[,-5],test = validaionset[,-5],cl=trainingset$class,k=round(sqrt(nrow(data)))) # K= the number of nearest neighbors to consider when making predictions.
```

```{r}
validaionset$class==model
#calculates the accuracy of the classifier on the validationset
sum(validaionset$class==model)/nrow(validaionset)
```

The accuracy is 88%.

#We calculate the accuracy of the classifier on the validationset by dividing the number of correctly predicted instances by the total number of instances in the validationset.

## leave one out procedure
```{r}
correct=0
round(sqrt(nrow(data))) #k=14
#result=numeric()
for(i in 1:nrow(data)){
  model=knn(train=data[-i,],test=data[i,],cl=data[-i,5],k=14) # cl= class labels
  if((model==data[i,5])==T) correct=correct+1
  #result[i]=model
}
correct
correct/nrow(data)
```

The accuracy is 99%.

## k-fold cross validation

```{r}
#Step-1 :-Shuffling the dataset randomly.
set.seed(1)
p=nrow(data)
index=sample(1:p,p,replace = F) 
index

#Step-2:- Split the dataset into k equally sized folds.
set1=data[index[1:40],]
set2=data[index[41:80],]
set3=data[index[81:120],]
set4=data[index[121:160],]
set5=data[index[161:199],]

```


```{r}
#Step-3:-For each fold i, train the model on all the other folds except fold i, and then evaluate the model on the data in fold i. Record the performance metric of interest for fold i.

#Step-4:-Repeat steps 3 for each fold.

model1=knn(train=rbind(set1,set2,set3,set4),test=set5,cl=rbind(set1,set2,set3,set4)$class,k=14)
accuracy1=sum(model1==set5$class)/dim(set5)[1]
#accuracy1

model2=knn(train=rbind(set1,set2,set3,set5),test=set4,cl=rbind(set1,set2,set3,set5)$class,k=14)
accuracy2=sum(model2==set4$class)/dim(set4)[1]
#model2

model3=knn(train=rbind(set1,set2,set5,set4),test=set3,cl=rbind(set1,set2,set5,set4)$class,k=14)
accuracy3=sum(model3==set3$class)/dim(set3)[1]
#accuracy3

model4=knn(train=rbind(set1,set5,set3,set4),test=set2,cl=rbind(set1,set5,set3,set4)$class,k=14)
accuracy4=sum(model4==set2$class)/dim(set2)[1]
#accuracy4

model5=knn(train=rbind(set5,set2,set3,set4),test=set1,cl=rbind(set5,set2,set3,set4)$class,k=14)
accuracy5=sum(model5==set1$class)/dim(set1)[1]
#accuracy5

#Step-5:-Calculate the average performance across all the folds.
mean(c(accuracy1,accuracy2,accuracy3,accuracy4,accuracy5))

```

The accuracy is 98.5%.


##Bootstrap

##Bootstrap estimate of standard error of data:-


-We are  performing bootstrap estimate of the mean of the data's Area values.

```{r}
set.seed(1)
B <- 200                  #the number of bootstrap samples to generate.

n <- nrow(data)                  #specifies the sample size

x<- data$Area
M <- numeric(B)     #creates an empty vector of length B to store the bootstrap sample means.

for(b in 1:B) 
{
   i <- sample(1:n, size=n, replace=TRUE)   #generates a bootstrap sample of size n by randomly sampling n observations with replacement from the original sample.
   M[b] <- mean(x[i])                        #Store the B bootstrap mean estimates in the M vector.
}
#original sample x,
print(x) 
#original sample x, mean of Area
print(mean(x))
#mean of the B bootstrap mean estimates 
print(mean(M))
#standard deviation of the B bootstrap mean estimates
print(sd(M))
```

The mean of the bootstrap mean estimates do not differ much from the mean of the original sample.




## Bootstrap estimate of bias


```{r}
B <- 200
n <- nrow(data)
x <- data$Area
M <- numeric(B)

for(b in 1:B) 
{
   i <- sample(1:n, size=n, replace=TRUE)
   M[b] <- mean(x[i])
}
#calculating the bias of the bootstrap estimate
bias <- mean(M)-mean(x)
print(bias)
```


#Bootstrap estimate of confidence interval

Consider the sample $x_{1}, \cdots, x_{n}$.
Let $\hat{\theta}$ be the sample mean, which is the estimate of the population mean $\theta$
underlying the sample.


```{r}
B <- 200    
n <- nrow(data)
x <- data$Area
M <- numeric(B)

for(b in 1:B) 
{
   i <- sample(1:n, size=n, replace=TRUE)
   M[b] <- mean(x[i])
}
#calculates the 95% percentile-based confidence interval for the population mean of the  variable using the bootstrap samples.
quantile(M, c(0.025, 0.975))
#calculates the 95% percentile-based confidence interval for the bias-corrected population mean of the variable using the bootstrap samples.
quantile(M-mean(x), c(0.025, 0.975))
```

If the mean of the sample data is the population mean, it lies in the confidence interval of the variable using bootstrap estimates.





```{r}
data= read.csv("seeds.csv")
str(data)
table(data$Type)
data=data[data$Type==1 | data$Type==2,]
table(data$Type)
#Choose either Type 1 seed or Type 2 Seed

data$Type= ifelse(data$Type==1,1,0) #Recode Type 1 seed to 1, Type 2 seed to 0
table(data$Type)
data$Type= as.factor(data$Type)
```


#Case1

```{r}
str(data)
q=5
co=rep(0,5)    
y=data$Type
y= as.character(y)
y=as.numeric((y))
s=data[,-ncol(data)]

p=7

## calculates the correlation between the ith column of the matrix s and the vector y
  
for (i in 1:p)
{
co[i]=cor(data[,i],y)       
}  

s1=s[,order(-abs(co))[1:(q)]]

s1=cbind(s1,y)    # Creating the new matrix with the 50 columns
s1
```


#performing 5-fold cross-validation to assess the performance of a logistic regression 


```{r}
acc1=rep(0,5)
for (i in 1:5)
{ 
test=s1[1:30,]
train=s1[31:134,]
# logistic regression
mod=glm(formula=(train$y)~.,data=train,family=binomial(link="logit"))
#Prdicting the model
prob=predict(mod,test,type="response")
label=rep('',length(prob))
label[prob<0.5]=0
label[prob>=0.5]=1
tab=table(predict=label,real=test$y)
#calculates the classification accuracy for the test data 
acc1[i]=sum(diag(tab))/sum(tab)
}
#the misclassification rate for each iteration.
1-acc1
#calculates the average misclassification rate across all iterations.
mean(1-acc1)

```

#Case2
```{r}
data
acc2=rep(0,5)
for (i in 1:5)
{ 
  test=data[1:30,]
  train=data[31:134,]
  mod=glm(formula=Type~.,data=train,family=binomial(link="logit"))
  prob=predict(mod,test,type="response")
  label=rep('',length(prob))
  label[prob<0.5]=0
  label[prob>=0.5]=1
  tab=table(predict=label,real=test$Type)
  acc2[i]=sum(diag(tab))/sum(tab)
}
1-acc2
mean(1-acc2)
```


##Case3
```{r}
q=q
acc3=rep(0,5)
for (i in 1:5)
{ 
s3=cbind(s,y)
test=s3[c((10*i-10+1):(10*i)),]
train=s3[-c((10*i-10+1):(10*i)),]
  co=rep(0,p)
  for (j in 1:p)
  {
  co[j]=cor(train[,j],train$y)  
  } 
f=c(order(-abs(co))[1:(q)],p+1)
test=test[,f]
train=train[,f]
mod=glm(formula=y~.,data=train,family=binomial(link="logit"))
prob=predict(mod,test,type="response")
label=rep('',length(prob))
label[prob<0.5]=0
label[prob>=0.5]=1
tab=table(predict=label,real=test$y)
acc3[i]=sum(diag(tab))/sum(tab)
}
1-acc3
mean(1-acc3)
```

```{r}
q=2
#install.packages('sigmoid')
library(sigmoid)
p=5000
n=50
s=sample(c(-1,1),n*(p),replace=T)
s=matrix(s,ncol=p)
s=as.data.frame(s)
y=sigmoid(rowSums(s[,1:q]))
y[y>=0.5]=1
y[y<0.5]=0
```

```{r}
q=q
s4=cbind(s,y)
acc4=rep(0,5)
for (i in 1:5)
{ 
test=s4[c((10*i-10+1):(10*i)),]
train=s4[-c((10*i-10+1):(10*i)),]
  co=rep(0,p)
  for (j in 1:p)
  {
  co[j]=cor(train[,j],train$y)  
  } 
f=c(order(-abs(co))[1:(q)],p+1)
test=test[,f]
train=train[,f]
mod=glm(formula=y~.,data=train,family=binomial(link="logit"))
prob=predict(mod,test,type="response")
label=rep('',length(prob))
label[prob<0.5]=0
label[prob>=0.5]=1
tab=table(predict=label,real=test$y)
acc4[i]=sum(diag(tab))/sum(tab)
}
1-acc4
mean(1-acc4)
```

```{r}
#install.packages('leaps')
library(leaps)
regfit.full = regsubsets(y ~ ., data = s4,nvmax=4, method ="forward")
coef(regfit.full ,4)
```


